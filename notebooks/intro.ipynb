{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Torch Spatio-Temporal (TSL)\n",
    "official documentation: https://torch-spatiotemporal.readthedocs.io/en/latest/index.html\n",
    "\n",
    "purpose of this notebook:\n",
    "- data structures\n",
    "- datasets\n",
    "- models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsl\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "from torch.optim import Adam\n",
    "from tsl.datasets import PeMS04, PeMS07, PeMS08, PemsBay\n",
    "from tsl.datasets import MetrLA\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsl version  : 0.9.5\n",
      "torch version: 2.4.0\n",
      "torch_geometric version: 2.6.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"tsl version  : {tsl.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torch_geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling period: <5 * Minutes>\n",
      "Has missing values: True\n",
      "Percentage of missing values: 8.11%\n",
      "Has exogenous variables: True\n",
      "Covariates: dist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch-st/lib/python3.10/site-packages/tsl/datasets/metr_la.py:98: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(df.index[0], df.index[-1], freq='5T')\n",
      "/opt/anaconda3/envs/torch-st/lib/python3.10/site-packages/tsl/datasets/metr_la.py:109: FutureWarning: The 'method' keyword in DataFrame.replace is deprecated and will be removed in a future version.\n",
      "  df = df.replace(to_replace=0., method='ffill')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MetrLA(length=34272, n_nodes=207, n_channels=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_MetrLA = MetrLA(root='data/MetrLA')\n",
    "\n",
    "print(f\"Sampling period: {dataset_MetrLA.freq}\")\n",
    "print(f\"Has missing values: {dataset_MetrLA.has_mask}\")\n",
    "print(f\"Percentage of missing values: {(1 - dataset_MetrLA.mask.mean()) * 100:.2f}%\")\n",
    "print(f\"Has exogenous variables: {dataset_MetrLA.has_covariates}\")\n",
    "print(f\"Covariates: {', '.join(dataset_MetrLA.covariates.keys())}\")\n",
    "\n",
    "dataset_MetrLA  # type tsl.datasets.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.      inf     inf ...     inf  8114.8 10009.7]\n",
      " [    inf     0.   2504.6 ...     inf     inf     inf]\n",
      " [    inf  1489.3     0.  ...     inf     inf  9837. ]\n",
      " ...\n",
      " [    inf     inf     inf ...     0.      inf     inf]\n",
      " [ 9599.8     inf     inf ...     inf     0.      inf]\n",
      " [10119.9  9374.8     inf ...     inf  9018.7     0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_MetrLA.dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default similarity: distance\n",
      "Available similarity options: {'distance'}\n",
      "==========================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(207, 207)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Default similarity: {dataset_MetrLA.similarity_score}\")\n",
    "print(f\"Available similarity options: {dataset_MetrLA.similarity_options}\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "sim = dataset_MetrLA.get_similarity(\"distance\")  # or dataset_MetrLA.compute_similarity()\n",
    "\n",
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1515), (1515,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_connectivity uses get_similarity under the hood\n",
    "connectivity = dataset_MetrLA.get_connectivity(threshold=0.1, include_self=False, normalize_axis=1, layout=\"edge_index\")\n",
    "\n",
    "connectivity[0].shape, connectivity[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a torch-ready dataset\n",
    "\n",
    "convert dataset_MetrLA in (tsl.datasets.Dataset) format to (tsl.data.SpatioTemporalDataset) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.data import SpatioTemporalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>nodes</th>\n",
       "      <th>773869</th>\n",
       "      <th>767541</th>\n",
       "      <th>767542</th>\n",
       "      <th>717447</th>\n",
       "      <th>717446</th>\n",
       "      <th>717445</th>\n",
       "      <th>773062</th>\n",
       "      <th>767620</th>\n",
       "      <th>737529</th>\n",
       "      <th>717816</th>\n",
       "      <th>...</th>\n",
       "      <th>772167</th>\n",
       "      <th>769372</th>\n",
       "      <th>774204</th>\n",
       "      <th>769806</th>\n",
       "      <th>717590</th>\n",
       "      <th>717592</th>\n",
       "      <th>717595</th>\n",
       "      <th>772168</th>\n",
       "      <th>718141</th>\n",
       "      <th>769373</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channels</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:00:00</th>\n",
       "      <td>64.375000</td>\n",
       "      <td>67.625000</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>66.875000</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>65.125</td>\n",
       "      <td>67.125</td>\n",
       "      <td>59.625000</td>\n",
       "      <td>62.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>45.625000</td>\n",
       "      <td>65.500</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>66.428574</td>\n",
       "      <td>66.875</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>61.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:05:00</th>\n",
       "      <td>62.666668</td>\n",
       "      <td>68.555557</td>\n",
       "      <td>65.444443</td>\n",
       "      <td>62.444443</td>\n",
       "      <td>64.444443</td>\n",
       "      <td>68.111115</td>\n",
       "      <td>65.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>57.444443</td>\n",
       "      <td>63.333332</td>\n",
       "      <td>...</td>\n",
       "      <td>50.666668</td>\n",
       "      <td>69.875</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>58.555557</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>64.444443</td>\n",
       "      <td>55.888889</td>\n",
       "      <td>68.444443</td>\n",
       "      <td>62.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:10:00</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500</td>\n",
       "      <td>64.250</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857140</td>\n",
       "      <td>62.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:15:00</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500</td>\n",
       "      <td>64.250</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857140</td>\n",
       "      <td>62.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:20:00</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500</td>\n",
       "      <td>64.250</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857140</td>\n",
       "      <td>62.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "nodes                   773869     767541     767542     717447     717446  \\\n",
       "channels                     0          0          0          0          0   \n",
       "2012-03-01 00:00:00  64.375000  67.625000  67.125000  61.500000  66.875000   \n",
       "2012-03-01 00:05:00  62.666668  68.555557  65.444443  62.444443  64.444443   \n",
       "2012-03-01 00:10:00  64.000000  63.750000  60.000000  59.000000  66.500000   \n",
       "2012-03-01 00:15:00  64.000000  63.750000  60.000000  59.000000  66.500000   \n",
       "2012-03-01 00:20:00  64.000000  63.750000  60.000000  59.000000  66.500000   \n",
       "\n",
       "nodes                   717445  773062  767620     737529     717816  ...  \\\n",
       "channels                     0       0       0          0          0  ...   \n",
       "2012-03-01 00:00:00  68.750000  65.125  67.125  59.625000  62.750000  ...   \n",
       "2012-03-01 00:05:00  68.111115  65.000  65.000  57.444443  63.333332  ...   \n",
       "2012-03-01 00:10:00  66.250000  64.500  64.250  63.875000  65.375000  ...   \n",
       "2012-03-01 00:15:00  66.250000  64.500  64.250  63.875000  65.375000  ...   \n",
       "2012-03-01 00:20:00  66.250000  64.500  64.250  63.875000  65.375000  ...   \n",
       "\n",
       "nodes                   772167  769372     774204     769806  717590  \\\n",
       "channels                     0       0          0          0       0   \n",
       "2012-03-01 00:00:00  45.625000  65.500  64.500000  66.428574  66.875   \n",
       "2012-03-01 00:05:00  50.666668  69.875  66.666664  58.555557  62.000   \n",
       "2012-03-01 00:10:00  44.125000  69.000  56.500000  59.250000  68.125   \n",
       "2012-03-01 00:15:00  44.125000  69.000  56.500000  59.250000  68.125   \n",
       "2012-03-01 00:20:00  44.125000  69.000  56.500000  59.250000  68.125   \n",
       "\n",
       "nodes                   717592     717595     772168     718141  769373  \n",
       "channels                     0          0          0          0       0  \n",
       "2012-03-01 00:00:00  59.375000  69.000000  59.250000  69.000000  61.875  \n",
       "2012-03-01 00:05:00  61.111111  64.444443  55.888889  68.444443  62.875  \n",
       "2012-03-01 00:10:00  62.500000  65.625000  61.375000  69.857140  62.000  \n",
       "2012-03-01 00:15:00  62.500000  65.625000  61.375000  69.857140  62.000  \n",
       "2012-03-01 00:20:00  62.500000  65.625000  61.375000  69.857140  62.000  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset_MetrLA.dataframe()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatioTemporalDataset(n_samples=34249, n_nodes=207, n_channels=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subclass of torch.utils.data.Dataset\n",
    "torch_dataset = SpatioTemporalDataset(\n",
    "    target=dataset_MetrLA.dataframe(),\n",
    "    connectivity=connectivity,\n",
    "    mask=dataset_MetrLA.mask,\n",
    "    horizon=6,\n",
    "    window=18,\n",
    "    stride=1\n",
    ")\n",
    "\n",
    "torch_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([18, 207, 1]),\n",
       " Data(\n",
       "   input=(x=[t=18, n=207, f=1], edge_index=[2, e=1515], edge_weight=[e=1515]),\n",
       "   target=(y=[t=6, n=207, f=1]),\n",
       "   has_mask=True\n",
       " ))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch_dataset[0]\n",
    "\n",
    "\"\"\"\n",
    "    sample is of \n",
    "    - type torch_geometric.data.Data\n",
    "    - shape (window, num_nodes, num_features)\n",
    "\"\"\"\n",
    "sample.x.shape, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 't n f',\n",
       " 'mask': 't n f',\n",
       " 'edge_index': '2 e',\n",
       " 'edge_weight': 'e',\n",
       " 'y': 't n f'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    t = time steps dimension\n",
    "    n = node dimension\n",
    "    e = edge dimension\n",
    "    f = feature dimension\n",
    "    b = batch dimension\n",
    "\"\"\"\n",
    "sample.pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StaticBatch(\n",
       "  input=(x=[b=5, t=18, n=207, f=1], edge_index=[2, e=1515], edge_weight=[e=1515]),\n",
       "  target=(y=[b=5, t=6, n=207, f=1]),\n",
       "  has_mask=True\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch_dataset[:5]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Train dataloader: size=24642}\n",
      "{Validation dataloader: size=2722}\n",
      "{Test dataloader: size=6849}\n",
      "{Predict dataloader: None}\n"
     ]
    }
   ],
   "source": [
    "from tsl.data.datamodule import (SpatioTemporalDataModule,\n",
    "                                 TemporalSplitter)\n",
    "from tsl.data.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize data using mean and std computed over time and node dimensions\n",
    "scalers = {'target': StandardScaler(axis=(0, 1))}\n",
    "\n",
    "# Split data sequentially:\n",
    "#   |------------ dataset -----------|\n",
    "#   |--- train ---|- val -|-- test --|\n",
    "splitter = TemporalSplitter(val_len=0.1, test_len=0.2)\n",
    "\n",
    "dm = SpatioTemporalDataModule(\n",
    "    dataset=torch_dataset,\n",
    "    scalers=scalers,\n",
    "    splitter=splitter,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "print(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the loaders separately for training, validation and testing (for pytorch-lightning, dm can be plugged in as is)\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "test_loader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from tsl.nn.blocks.encoders import RNN\n",
    "from tsl.nn.layers import NodeEmbedding, DiffConv\n",
    "from einops.layers.torch import Rearrange  # reshape data with Einstein notation\n",
    "\n",
    "\n",
    "class TimeThenSpaceModel(nn.Module):\n",
    "    def __init__(self, input_size: int, n_nodes: int, horizon: int,\n",
    "                 hidden_size: int = 32,\n",
    "                 rnn_layers: int = 1,\n",
    "                 gnn_kernel: int = 2):\n",
    "        super(TimeThenSpaceModel, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.node_embeddings = NodeEmbedding(n_nodes, hidden_size) # free params learned individually for each node (taming local effects in STGNNs, Cini et al.)\n",
    "\n",
    "        self.time_nn = RNN(input_size=hidden_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           n_layers=rnn_layers,\n",
    "                           cell='gru',\n",
    "                           return_only_last_state=True)\n",
    "        \n",
    "        self.space_nn = DiffConv(in_channels=hidden_size,\n",
    "                                 out_channels=hidden_size,\n",
    "                                 k=gnn_kernel)\n",
    "\n",
    "        self.decoder = nn.Linear(hidden_size, input_size * horizon)\n",
    "        self.rearrange = Rearrange('b n (t f) -> b t n f', t=horizon)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        # x: [batch time nodes features]\n",
    "        x_enc = self.encoder(x)  # linear encoder: x_enc = xΘ + b\n",
    "        x_emb = x_enc + self.node_embeddings()  # add node-identifier embeddings\n",
    "        h = self.time_nn(x_emb)  # temporal processing: x=[b t n f] -> h=[b n f]\n",
    "        z = self.space_nn(h, edge_index, edge_weight)  # spatial processing\n",
    "        x_out = self.decoder(z)  # linear decoder: z=[b n f] -> x_out=[b n t⋅f]\n",
    "        x_horizon = self.rearrange(x_out)\n",
    "        return x_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeThenSpaceModel(\n",
      "  (encoder): Linear(in_features=1, out_features=32, bias=True)\n",
      "  (node_embeddings): NodeEmbedding(n_nodes=207, embedding_size=32)\n",
      "  (time_nn): RNN(\n",
      "    (rnn): GRU(32, 32)\n",
      "  )\n",
      "  (space_nn): DiffConv(32, 32)\n",
      "  (decoder): Linear(in_features=32, out_features=6, bias=True)\n",
      "  (rearrange): Rearrange('b n (t f) -> b t n f', t=6)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 32   #@param\n",
    "rnn_layers = 1     #@param\n",
    "gnn_kernel = 2     #@param\n",
    "\n",
    "input_size = torch_dataset.n_channels   # 1 channel\n",
    "n_nodes = torch_dataset.n_nodes         # 207 nodes\n",
    "horizon = torch_dataset.horizon         # 12 time steps\n",
    "\n",
    "stgnn = TimeThenSpaceModel(input_size=input_size,\n",
    "                           n_nodes=n_nodes,\n",
    "                           horizon=horizon,\n",
    "                           hidden_size=hidden_size,\n",
    "                           rnn_layers=rnn_layers,\n",
    "                           gnn_kernel=gnn_kernel)\n",
    "print(stgnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - loss: 75.9901\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(stgnn.parameters(), lr=1e-3)\n",
    "\n",
    "stgnn.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, edge_index, edge_weight, y = batch.x, batch.edge_index, batch.edge_weight, batch.y\n",
    "        \n",
    "        y_hat = stgnn(x, edge_index, edge_weight)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.metrics.torch import MaskedMAE, MaskedMAPE\n",
    "from tsl.engines import Predictor\n",
    "\n",
    "loss_fn = MaskedMAE()\n",
    "\n",
    "metrics = {'mae': MaskedMAE(),\n",
    "           'mape': MaskedMAPE(),\n",
    "           'mae_at_15': MaskedMAE(at=2),  # '2' indicates the third time step,\n",
    "                                          # which correspond to 15 minutes ahead\n",
    "           'mae_at_30': MaskedMAE(at=5),\n",
    "           'mae_at_60': MaskedMAE(at=11)}\n",
    "\n",
    "# setup predictor\n",
    "predictor = Predictor(\n",
    "    model=stgnn,                   # our initialized model\n",
    "    optim_class=torch.optim.Adam,  # specify optimizer to be used...\n",
    "    optim_kwargs={'lr': 0.001},    # ...and parameters for its initialization\n",
    "    loss_fn=loss_fn,               # which loss function to be used\n",
    "    metrics=metrics                # metrics to be logged during train/val/test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/torch-st/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/HunJ/Documents/ResearchLab/CodeBases/temporal-gnn/notebooks/logs exists and is not empty.\n",
      "\n",
      "  | Name          | Type               | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | loss_fn       | MaskedMAE          | 0      | train\n",
      "1 | train_metrics | MetricCollection   | 0      | train\n",
      "2 | val_metrics   | MetricCollection   | 0      | train\n",
      "3 | test_metrics  | MetricCollection   | 0      | train\n",
      "4 | model         | TimeThenSpaceModel | 18.4 K | train\n",
      "-------------------------------------------------------------\n",
      "18.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.4 K    Total params\n",
      "0.073     Total estimated model params size (MB)\n",
      "29        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch-st/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only args ['edge_weight', 'x', 'edge_index'] are forwarded to the model (TimeThenSpaceModel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch-st/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:07<00:00, 12.55it/s, v_num=3, val_mae=6.900, val_mae_at_15=6.540, val_mae_at_30=6.530, val_mae_at_60=0.000, val_mape=0.150, train_mae=202.0, train_mae_at_15=202.0, train_mae_at_30=202.0, train_mae_at_60=0.000, train_mape=3.690]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:07<00:00, 12.52it/s, v_num=3, val_mae=6.900, val_mae_at_15=6.540, val_mae_at_30=6.530, val_mae_at_60=0.000, val_mape=0.150, train_mae=202.0, train_mae_at_15=202.0, train_mae_at_30=202.0, train_mae_at_60=0.000, train_mape=3.690]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='logs',\n",
    "    save_top_k=1,\n",
    "    monitor='val_mae',\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1,\n",
    "                     limit_train_batches=100,  # end an epoch after 100 updates\n",
    "                     callbacks=[checkpoint_callback])\n",
    "\n",
    "trainer.fit(predictor, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch-st/lib/python3.10/site-packages/tsl/engines/predictor.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  storage = torch.load(filename, lambda storage, loc: storage)\n",
      "Predictor with already instantiated model is loading a state_dict from /Users/HunJ/Documents/ResearchLab/CodeBases/temporal-gnn/notebooks/logs/epoch=0-step=100-v1.ckpt. Cannot  check if model hyperparameters are the same.\n",
      "/opt/anaconda3/envs/torch-st/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 108/108 [00:03<00:00, 30.79it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss            7.02519416809082\n",
      "        test_mae             7.313732624053955\n",
      "     test_mae_at_15          6.849016189575195\n",
      "     test_mae_at_30          7.109511375427246\n",
      "     test_mae_at_60                 0.0\n",
      "        test_mape           0.1687723696231842\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "predictor.load_model(checkpoint_callback.best_model_path)\n",
    "predictor.freeze()\n",
    "\n",
    "trainer.test(predictor, datamodule=dm);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
