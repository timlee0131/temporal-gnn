{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import math\n",
    "\n",
    "from torch.optim import Adam\n",
    "from tsl.datasets import PeMS04, PeMS07, PeMS08, PemsBay\n",
    "from tsl.datasets import MetrLA\n",
    "from tsl.data import SpatioTemporalDataset\n",
    "from tsl.data.datamodule import (SpatioTemporalDataModule,\n",
    "                                 TemporalSplitter)\n",
    "from tsl.data.preprocessing import StandardScaler\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/hunjael/.conda/envs/torch-st/lib/python3.10/site-packages/tsl/datasets/metr_la.py:98: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  date_range = pd.date_range(df.index[0], df.index[-1], freq='5T')\n",
      "/users/hunjael/.conda/envs/torch-st/lib/python3.10/site-packages/tsl/datasets/metr_la.py:109: FutureWarning: The 'method' keyword in DataFrame.replace is deprecated and will be removed in a future version.\n",
      "  df = df.replace(to_replace=0., method='ffill')\n"
     ]
    }
   ],
   "source": [
    "dataset_MetrLA = MetrLA(root='data/MetrLA')\n",
    "\n",
    "# get_connectivity uses get_similarity under the hood\n",
    "connectivity = dataset_MetrLA.get_connectivity(threshold=0.1, include_self=False, normalize_axis=1, layout=\"edge_index\")\n",
    "\n",
    "# subclass of torch.utils.data.Dataset\n",
    "torch_dataset = SpatioTemporalDataset(\n",
    "    target=dataset_MetrLA.dataframe(),\n",
    "    connectivity=connectivity,\n",
    "    mask=dataset_MetrLA.mask,\n",
    "    horizon=6,\n",
    "    window=12,\n",
    "    stride=1\n",
    ")\n",
    "\n",
    "scalers = {'target': StandardScaler(axis=(0, 1))}\n",
    "\n",
    "# Split data sequentially:\n",
    "#   |------------ dataset -----------|\n",
    "#   |--- train ---|- val -|-- test --|\n",
    "splitter = TemporalSplitter(val_len=0.1, test_len=0.2)\n",
    "\n",
    "dm = SpatioTemporalDataModule(\n",
    "    dataset=torch_dataset,\n",
    "    scalers=scalers,\n",
    "    splitter=splitter,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "test_loader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StaticBatch(\n",
       "  input=(x=[b=10, t=12, n=207, f=1], edge_index=[2, e=1515], edge_weight=[e=1515]),\n",
       "  target=(y=[b=10, t=6, n=207, f=1]),\n",
       "  has_mask=True,\n",
       "  transform=[x, y]\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = torch_dataset[:10]\n",
    "sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(1, 2, 12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the classic sinusoidal positional encoding.\n",
    "    \n",
    "    For an input tensor of shape (B, T, d_model) or (B, N, T, d_model),\n",
    "    it adds a positional encoding to every token along the time axis.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the token embeddings.\n",
    "            dropout (float): Dropout rate applied after adding PE.\n",
    "            max_len (int): Maximum sequence length to precompute positional encoding.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a (max_len, d_model) matrix; each row is the positional encoding for that time step.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute the division term using the logarithm of 10000 (a common choice)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * \n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)  # pe is not a parameter, but persistent\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input embedding with shape either (B, T, d_model)\n",
    "                              or (B, N, T, d_model).\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after adding positional encodings.\n",
    "        \"\"\"\n",
    "        if x.dim() == 3:\n",
    "            # x has shape (B, T, d_model)\n",
    "            x = x + self.pe[:, :x.size(1)]\n",
    "        elif x.dim() == 4:\n",
    "            # x has shape (B, N, T, d_model). Expand pe to (1,1,T,d_model) and add along the time dimension.\n",
    "            x = x + self.pe[:, :x.size(2)].unsqueeze(1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported input dimension for PositionalEncoding\")\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, window_size, num_heads=8, dropout=0.6):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.pe = PositionalEncoding(hidden_size)\n",
    "\n",
    "        self.self_q = nn.Linear(hidden_size, hidden_size * num_heads)\n",
    "        self.self_k = nn.Linear(hidden_size, hidden_size * num_heads)\n",
    "        self.self_v = nn.Linear(hidden_size, hidden_size * num_heads)\n",
    "        \n",
    "        self.cross_q = nn.Linear(hidden_size, hidden_size * num_heads)\n",
    "        self.cross_k = nn.Linear(hidden_size, hidden_size * num_heads)\n",
    "        self.cross_v = nn.Linear(hidden_size, hidden_size * num_heads)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size * num_heads, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = hidden_size ** 0.5\n",
    "        causal_mask = torch.triu(torch.ones(window_size, window_size), diagonal=1).bool()\n",
    "        self.register_buffer('causal_mask', causal_mask)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        B, N, T, D = x.size()\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.pe(x)\n",
    "        print(\"x pe: \", x.size())\n",
    "        \n",
    "        Q_self = self.self_q(x).view(B, N, T, self.num_heads, self.hidden_size).transpose(2, 3)\n",
    "        K_self = self.self_k(x).view(B, N, T, self.num_heads, self.hidden_size).transpose(2, 3)\n",
    "        V_self = self.self_v(x).view(B, N, T, self.num_heads, self.hidden_size).transpose(2, 3)\n",
    "        \n",
    "        e_self = (Q_self @ K_self.mT) / self.scale\n",
    "        e_self = e_self.masked_fill(self.causal_mask, float('-inf'))\n",
    "        print(\"mask size: \", self.causal_mask.size())\n",
    "        print(\"e_self: \", e_self.size())\n",
    "        \n",
    "        attention_Self = F.softmax(e_self, dim=-1)\n",
    "        attention_Self = self.dropout(attention_Self)\n",
    "        out_self = attention_Self @ V_self\n",
    "        print(\"out_self: \", out_self.size())\n",
    "        \n",
    "        out_self = out_self.transpose(2, 3).contiguous().view(B, N, T, self.hidden_size * self.num_heads)\n",
    "        out_self = self.out(out_self)\n",
    "        print(\"out_self (after out proj): \", out_self.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x pe:  torch.Size([10, 207, 12, 4])\n",
      "mask size:  torch.Size([12, 12])\n",
      "e_self:  torch.Size([10, 207, 8, 12, 12])\n",
      "out_self:  torch.Size([10, 207, 8, 12, 4])\n",
      "out_self (after out proj):  torch.Size([10, 207, 12, 4])\n"
     ]
    }
   ],
   "source": [
    "STA = TemporalAttention(input_size=1, hidden_size=4, window_size=12, num_heads=8, dropout=0.6)\n",
    "STA(sample1.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative version\n",
    "class IterCrossAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, window_size, num_heads=8, dropout=0.6):\n",
    "        super(IterCrossAttention, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.cross_q = nn.Linear(input_size, hidden_size * num_heads)\n",
    "        self.cross_k = nn.Linear(input_size, hidden_size * num_heads)\n",
    "        self.cross_v = nn.Linear(input_size, hidden_size * num_heads)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size * num_heads, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = hidden_size ** 0.5\n",
    "        self.causal_mask = torch.triu(torch.ones(window_size, window_size), diagonal=1).bool()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        B, N, T, D = x.size()\n",
    "        \n",
    "        out_cross_list = []\n",
    "        for i in range(N):\n",
    "            x_i = x[:, i, :, :] # shape (B, T, D)\n",
    "            Q_i = self.cross_q(x_i).view(B, T, self.num_heads, self.hidden_size).transpose(1, 2) # shape (B, num_heads, T, hidden_size)\n",
    "            \n",
    "            N_i = [j for j in range(N) if j != i]\n",
    "            x_neighbors = x[:, N_i, :, :] # shape (B, N-1, T, D)\n",
    "            \n",
    "            K_j = self.cross_k(x_neighbors).view(B, (N-1) * T, self.num_heads, self.hidden_size).transpose(1, 2) # shape (B, num_heads, (N-1)*T, hidden_size)\n",
    "            V_j = self.cross_v(x_neighbors).view(B, (N-1) * T, self.num_heads, self.hidden_size).transpose(1, 2) # shape (B, num_heads, (N-1)*T, hidden_size)\n",
    "            \n",
    "            print(\"Q, K, V: \", Q_i.size(), K_j.size(), V_j.size())\n",
    "            \n",
    "            e_cross = (Q_i @ K_j.mT) / self.scale\n",
    "            print(\"e_cross: \", e_cross.size())\n",
    "            \n",
    "            # expand the causal mask. for each node, we have (N-1) blocks each of size (T * T)\n",
    "            expanded_mask = self.causal_mask.repeat(1, (N-1))\n",
    "            e_cross = e_cross.masked_fill(expanded_mask, float('-inf'))\n",
    "            \n",
    "            attention_cross = F.softmax(e_cross, dim=-1)\n",
    "            attention_cross = self.dropout(attention_cross)\n",
    "            \n",
    "            out_i = attention_cross @ V_j\n",
    "            print(\"out_i: \", out_i.size())\n",
    "            out_i = out_i.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.hidden_size)\n",
    "            out_i = self.out(out_i)\n",
    "            print(\"out_i (after out proj): \", out_i.size())\n",
    "            \n",
    "            out_cross_list.append(out_i)\n",
    "        \n",
    "        out_cross = torch.stack(out_cross_list, dim=1) # shape (B, N, T, hidden_size)\n",
    "        print(\"out_cross: \", out_cross.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q, K, V:  torch.Size([1, 8, 12, 4]) torch.Size([1, 8, 12, 4]) torch.Size([1, 8, 12, 4])\n",
      "e_cross:  torch.Size([1, 8, 12, 12])\n",
      "out_i:  torch.Size([1, 8, 12, 4])\n",
      "out_i (after out proj):  torch.Size([1, 12, 4])\n",
      "Q, K, V:  torch.Size([1, 8, 12, 4]) torch.Size([1, 8, 12, 4]) torch.Size([1, 8, 12, 4])\n",
      "e_cross:  torch.Size([1, 8, 12, 12])\n",
      "out_i:  torch.Size([1, 8, 12, 4])\n",
      "out_i (after out proj):  torch.Size([1, 12, 4])\n",
      "out_cross:  torch.Size([1, 2, 12, 4])\n"
     ]
    }
   ],
   "source": [
    "ic_attention = IterCrossAttention(input_size=1, hidden_size=4, window_size=12, num_heads=8, dropout=0.6)\n",
    "ic_attention(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_mask(edge_index, edge_weight, N, T, mask_self=False):\n",
    "    \"\"\"\n",
    "    Constructs a combined mask of shape (N*T, N*T) that embeds:\n",
    "      - A temporal causal mask (T x T), and\n",
    "      - A graph connectivity mask derived from edge_index and edge_weight.\n",
    "    \n",
    "    For every pair of nodes, if a connection exists (edge_weight > 0),\n",
    "    the corresponding (T, T) block equals the temporal mask;\n",
    "    otherwise, it is set to -inf.\n",
    "    \n",
    "    Arguments:\n",
    "      edge_index: LongTensor of shape (2, E) where each column is (source, target).\n",
    "      edge_weight: Tensor of shape (E,) with positive weights (e.g. 1).\n",
    "      N: Number of nodes.\n",
    "      T: Number of time steps.\n",
    "      device: torch.device.\n",
    "    \n",
    "    Returns:\n",
    "      A mask of shape (N*T, N*T) to be added to the attention scores.\n",
    "    \"\"\"\n",
    "    # Build a dense connectivity indicator of shape (N, N)\n",
    "    # For each edge (u->v): set A[v,u] = edge_weight (so only when there is an edge, we want to allow attention).\n",
    "    A = torch.zeros((N, N))\n",
    "    A[edge_index[1], edge_index[0]] = edge_weight  # Note: our convention: edge (u, v) means u -> v\n",
    "\n",
    "    # Build a connectivity mask: if A > 0, we allow attention (0 added), else, we want to block by setting to -inf.\n",
    "    connectivity_indicator = torch.where(A > 0, torch.zeros_like(A), torch.full_like(A, float('-inf')))\n",
    "    \n",
    "    if mask_self:\n",
    "        # Mask self-attention: we do not want nodes to attend to themselves.\n",
    "        connectivity_indicator.fill_diagonal_(float('-inf'))\n",
    "    \n",
    "    # Create a dense temporal (causal) mask of shape (T, T)\n",
    "    # For example, we use a lower-triangular mask to prevent query at time t from attending to keys at future times\n",
    "    temporal_mask = torch.triu(torch.ones((T, T)) * float('-inf'), diagonal=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Now, we “lift” these masks to the full (N*T, N*T) mask.\n",
    "    For the graph part, we create a block mask using a Kronecker product.\n",
    "    The idea is:\n",
    "       mask = kron(connectivity_indicator, ones(T, T)) + kron(ones(N, N), temporal_mask)\n",
    "    For a connected node pair, connectivity_indicator==0 so that block becomes 0 + temporal_mask,\n",
    "    and for an unconnected pair, connectivity_indicator==-inf so block remains -inf.\n",
    "    \"\"\"\n",
    "    mask = torch.kron(connectivity_indicator, torch.ones((T, T))) + torch.kron(torch.ones((N, N)), temporal_mask)\n",
    "    \n",
    "    return mask.bool()  # shape: (N*T, N*T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative version\n",
    "class SpatioTemporalAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_nodes, window_size, edge_index, edge_weight, num_heads=8, dropout=0.6, mask_self=False):\n",
    "        super(SpatioTemporalAttention, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.pe = PositionalEncoding(hidden_size)\n",
    "        \n",
    "        self.cross_q = nn.Linear(hidden_size, hidden_size * num_heads)\n",
    "        self.cross_k = nn.Linear(hidden_size, hidden_size * num_heads)\n",
    "        self.cross_v = nn.Linear(hidden_size, hidden_size * num_heads)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size * num_heads, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = hidden_size ** 0.5\n",
    "        \n",
    "        # includes self-loop for self-attention + causal mask for ST attention\n",
    "        # self.causal_mask = torch.triu(torch.ones(num_nodes * window_size, num_nodes * window_size), diagonal=1).bool()\n",
    "        self.mask = build_combined_mask(edge_index, edge_weight, num_nodes, window_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        B, N, T, D = x.size()\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.pe(x)\n",
    "        \n",
    "        # (B, N, T, D) -> (B, N, T, num_heads, hidden_size) -> (B, num_heads, N, T, hidden_size)\n",
    "        Q = self.cross_q(x).view(B, N, T, self.num_heads, self.hidden_size).transpose(1, 3)\n",
    "        K = self.cross_k(x).view(B, N, T, self.num_heads, self.hidden_size).transpose(1, 3)\n",
    "        V = self.cross_v(x).view(B, N, T, self.num_heads, self.hidden_size).transpose(1, 3)\n",
    "        \n",
    "        print(\"Q, K, V: \", Q.size(), K.size(), V.size())\n",
    "        \n",
    "        # (B, num_heads, N * T, hidden_size)\n",
    "        Q_flat = Q.reshape(B, self.num_heads, N * T, self.hidden_size)\n",
    "        K_flat = K.reshape(B, self.num_heads, N * T, self.hidden_size)\n",
    "        V_flat = V.reshape(B, self.num_heads, N * T, self.hidden_size)\n",
    "        \n",
    "        print(\"Q_flat, K_flat, V_flat: \", Q_flat.size(), K_flat.size(), V_flat.size())\n",
    "        \n",
    "        e = Q_flat @ K_flat.mT / self.scale # (B, num_heads, N * T, N * T)\n",
    "        e = e.masked_fill(self.mask, float('-inf'))\n",
    "        \n",
    "        print(\"e: \", e.size())\n",
    "        print(\"mask size: \", self.mask.size())\n",
    "        \n",
    "        attention = F.softmax(e, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        out_flat = attention @ V_flat # (B, num_heads, N * T, hidden_size)\n",
    "        print(\"out_flat: \", out_flat.size())\n",
    "        out = out_flat.reshape(B, self.num_heads, N, T, self.hidden_size).transpose(1,3) # (B, N, T, num_heads, hidden_size)\n",
    "        print(\"out: \", out.size())\n",
    "        out = out.contiguous().view(B, N, T, self.num_heads * self.hidden_size)\n",
    "        out = self.out(out)\n",
    "        print(\"out (after out proj): \", out.size())\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data:  torch.Size([10, 12, 207, 1])\n",
      "Q, K, V:  torch.Size([10, 8, 12, 207, 4]) torch.Size([10, 8, 12, 207, 4]) torch.Size([10, 8, 12, 207, 4])\n",
      "Q_flat, K_flat, V_flat:  torch.Size([10, 8, 2484, 4]) torch.Size([10, 8, 2484, 4]) torch.Size([10, 8, 2484, 4])\n",
      "e:  torch.Size([10, 8, 2484, 2484])\n",
      "mask size:  torch.Size([2484, 2484])\n",
      "out_flat:  torch.Size([10, 8, 2484, 4])\n",
      "out:  torch.Size([10, 12, 207, 8, 4])\n",
      "out (after out proj):  torch.Size([10, 207, 12, 4])\n"
     ]
    }
   ],
   "source": [
    "# edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n",
    "# edge_weight = torch.tensor([1, 1], dtype=torch.float)\n",
    "edge_index = sample1.edge_index\n",
    "edge_weight = sample1.edge_weight\n",
    "\n",
    "sta = SpatioTemporalAttention(input_size=1, hidden_size=4, num_nodes=sample1.x.shape[2], window_size=12, edge_index=edge_index, edge_weight=edge_weight, num_heads=8, dropout=0.6)\n",
    "print(\"original data: \", sample1.x.size())\n",
    "out = sta(sample1.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse mask build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse mask indices shape: torch.Size([2, 468])\n",
      "Sparse mask values shape: torch.Size([468])\n"
     ]
    }
   ],
   "source": [
    "def build_sparse_combined_mask(edge_index, edge_weight, N, T, device, mask_self=False):\n",
    "    \"\"\"\n",
    "    Constructs a sparse combined mask of shape (N*T, N*T) that fuses:\n",
    "      - A temporal (causal) mask (only allowing keys at or before each time step), and \n",
    "      - A graph connectivity mask based on edge_index and edge_weight.\n",
    "    \n",
    "    For each valid edge (from source u to target v) – where by convention we \n",
    "    set A[v,u] from edge_index and edge_weight – we allow a block of size T×T \n",
    "    corresponding to that node pair. Within the block only the lower-triangular \n",
    "    positions (t_query >= t_key) are allowed (with value 0, meaning no penalty). \n",
    "    All other positions should be (conceptually) -inf.\n",
    "    \n",
    "    Args:\n",
    "        edge_index : LongTensor of shape (2, E). Each column is an edge (u, v)\n",
    "                     meaning node u feeds into node v.\n",
    "        edge_weight: Tensor of shape (E,) with positive weights.\n",
    "        N          : Number of nodes.\n",
    "        T          : Number of time steps.\n",
    "        device     : torch.device.\n",
    "        mask_self  : If True, even if an edge exists for a self-connection,\n",
    "                     its block will be entirely disallowed.\n",
    "    \n",
    "    Returns:\n",
    "        sparse_mask: A sparse COO tensor of shape (N*T, N*T) that stores\n",
    "                     the allowed positions (with value 0). When applying the mask,\n",
    "                     one should treat missing entries as -inf.\n",
    "    \"\"\"\n",
    "    # Filter out self-connections if needed\n",
    "    if mask_self:\n",
    "        valid = edge_index[0] != edge_index[1]\n",
    "        valid_edges = edge_index[:, valid]\n",
    "    else:\n",
    "        valid_edges = edge_index\n",
    "\n",
    "    E = valid_edges.shape[1]\n",
    "\n",
    "    # Obtain lower-triangular indices for a T x T block.\n",
    "    # These indices indicate positions where t_query >= t_key.\n",
    "    # tril_indices returns a tensor of shape (2, L) where L = T*(T+1)//2.\n",
    "    tril = torch.tril_indices(T, T, device=device)  # shape (2, L)\n",
    "    L_val = tril.shape[1]  # number of allowed temporal positions per block\n",
    "\n",
    "    # For each valid edge, the block corresponds to:\n",
    "    #   Row block:   target node's time steps, i.e., indices: v * T + t_query.\n",
    "    #   Column block: source node's time steps, i.e., indices: u * T + t_key.\n",
    "    # valid_edges[0] are source node indices, valid_edges[1] are target node indices.\n",
    "    # We use broadcasting to generate all allowed indices.\n",
    "\n",
    "    # Compute row indices: (E, L) tensor where each row corresponds to a valid edge.\n",
    "    row_indices = valid_edges[1].unsqueeze(1) * T + tril[0].unsqueeze(0)\n",
    "    # Compute column indices in the same way.\n",
    "    col_indices = valid_edges[0].unsqueeze(1) * T + tril[1].unsqueeze(0)\n",
    "\n",
    "    # Flatten the indices so that each valid edge contributes L_val entries.\n",
    "    row_indices = row_indices.reshape(-1)\n",
    "    col_indices = col_indices.reshape(-1)\n",
    "    sparse_indices = torch.stack([row_indices, col_indices], dim=0)  # shape (2, E * L_val)\n",
    "\n",
    "    # The allowed positions (in the causal part) get a value equal to the temporal mask.\n",
    "    # With our construction, the temporal mask is defined as:\n",
    "    #    0 if t_query >= t_key, and -inf otherwise.\n",
    "    # Here we only store positions where t_query >= t_key, so we set value 0.\n",
    "    values = torch.zeros(E * L_val, device=device)\n",
    "\n",
    "    # Construct the sparse mask—in our sparse representation, missing entries are implicitly 0.\n",
    "    # To treat missing entries as -inf during attention, later on you can compute:\n",
    "    #   combined_mask = dense_full + sparse_mask.to_dense()\n",
    "    # where dense_full is a (N*T, N*T) tensor filled with -inf.\n",
    "    sparse_mask = torch.sparse_coo_tensor(sparse_indices, values, size=(N*T, N*T), device=device)\n",
    "    return sparse_mask\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    N = 5    # number of nodes\n",
    "    T = 12   # number of time steps\n",
    "\n",
    "    # Example edge_index: shape (2, E)\n",
    "    # Here, edge (u, v) means node u's features are used to attend to node v.\n",
    "    edge_index = torch.tensor([\n",
    "        [0, 2, 3, 1, 4, 0],  # source nodes\n",
    "        [1, 0, 4, 4, 2, 3]   # target nodes\n",
    "    ], dtype=torch.long, device=device)\n",
    "    edge_weight = torch.ones(edge_index.shape[1], device=device)\n",
    "\n",
    "    # Build sparse mask; set mask_self=True to mask self-attention.\n",
    "    sparse_mask = build_sparse_combined_mask(edge_index, edge_weight, N, T, device, mask_self=True)\n",
    "    print(\"Sparse mask indices shape:\", sparse_mask._indices().shape)\n",
    "    print(\"Sparse mask values shape:\", sparse_mask._values().shape)\n",
    "\n",
    "    # Note: When applying this mask to attention scores (of shape (B, num_heads, N*T, N*T)),\n",
    "    # you may want to convert this sparse mask to a dense tensor and then add it to the attention scores:\n",
    "    #   dense_mask = torch.full((N*T, N*T), float('-inf'), device=device)\n",
    "    #   dense_mask = dense_mask + sparse_mask.to_dense()\n",
    "    # Or, if you can leverage a sparse-aware attention function, have it interpret missing entries as -inf.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
