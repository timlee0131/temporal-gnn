# Spatio Temporal Time Series Processing with Graph Neural Networks

Modeling of multivariate time series with spatial inductive bias mostly in the domain of traffic flow prediction

I mainly investigated attention based models like transformers

Defining the types of attention that goes into a fully-connected Spatio Temporal Model (these are my definitions and may be called something else in the literature)
1. strictly temporal attention: self-attention along the temporal dimension within each node. In a STGNN setting, strictly temporal attention should be coupled with a GNN for spatial aggregation
2. static spatial attention: node-level attention along the spatial dimension (i.e. standard GAT). This type of attention is only capable of attending to other nodes at the current time step. Combined with temporal attention, each time-step learns information about its own univariate series information and performs message passing to other nodes at the current time step. Given oversmoothing and oversquashing tendencies of message passing, not being able to attend to various (past) time steps of one's neighbors can be a limiting factor. 
3. dynamic spatial attention: node-level attention along the spatial dimension that attends to neighboring nodes not just at the current time-step but their previous time-steps as well. This allows each node to capture neighboring nodes' historical patterns directly without relying on aggregated information at the current step being passed.
4. dynamic spatio temporal attention: attention that combines temporal attention with dynamic spatial attention. In the most basic implementation (with connectivity masking), it would have $O(NKT^2d)$ time complexity and $O(NKT^2)$ memory complexity where $N$ is the number of nodes (channels), $K$ is the average degree, and $T$ is the number of time steps.

### Models Implemented

**Dynamic Spatio Temporal Attention Paradigm**: modeling intra-node (time) and inter-node (space) information together without explicitly separating temporal and spatial elements (computationally more intensive than TTS)
- Full dynamic spatio-temporal transformer: Every time-step at each node connects to every other time-step at every node (with causal masking for temporal causality). This is the most naive and straightforward implementation of a transformer into the multivariate time series domain, but nontheless there are research papers that attempt to make this work. This model requires a very expensive Kronecker product operation to create a causal mask of size $\mathbb{R}^{NT * NT}$ and has the overall complexity of $O(N^2T^2)$. In addition to being one of the most computationally intensive model possible, it doesn't even work that well since this gargantuan transformer collects too much noise and irrelevant patterns in the data.
- Dynamic Spatio Temporal Attention Networks (DSTANs): My proposed line of upgrades and improvements from the naive full transformer. DSTAN variants I create all use dynamic spatio temporal attention to interact with neiboring time steps not just in the current time step but also into the past to capture multi-step dependencies and long-term patterns. To achieve dynamic spatial attention, various inter-node attention schemes are used in conjunction with a standard graph attention layer to not only attend to each neighbor's past window but to weigh each neighbor's overall contribution strength for a given source node. 
    - DSTAConv: Uses linear attention for inter-node attention from the paper (Transformers are RNNs, Katharopoulos 2020). Intra-node temporal attention is also computed simultaneously via added self-loops.
    - DSTAfullConv: Uses full softmax attention for both inter-node and intra-node attention. Did not see major improvements from DSTAConv.

**Time-then-Space Paradigm (TTS)**: models that process temporal information in a univariate fashion and then performs inter-node message passing to get information from neighbors (equivalent to space-then-time in complexity and shown to have no performance diff)
- Recurrent Neural Network (time) + Graph Convolution Network (space)
- Transformer (time) + Graph Attention Network (space)

### My Observations
While my methods got pretty close to SOTA performance on tested benchmarks (mainly tested on traffic flow prediction tasks like MetrLA and PemsBay), I noticed that model complexity and theoretical capacity did not translate well to performance. If anything, more complex and heavy-duty attention/transformer schemes performed worse in addition to being memory and time inefficient. A simple TTS model with RNN + GCN performed on par with my attention networks for the most part. I have identified several key issues.
1. **Point-wise attention does not work well in time-series data**: Unlike language where each token consists of one word (or even less), individual time steps in time-series data do not hold much semantic meaning. This is fully investigated in papers such as (PatchTST, Nie & Nguyen 2023). In effect, my strategy to perform point-wise attention where individual time steps were attending with individual time steps was actually significantly less expressive than I hypothesized.
2. **Naive implementation of attention is very costly in multivariate time series**: Building off of observation #1, complexity and computation do not equal to more expressivity and can be very costly in memory and computation (perhaps even more so than in other domains). In multivariate time series, a transformer incurs the quadratic complexity not just in the number of nodes (variables/features/channels), but also in the number of time steps. While I still believe that attention-based methods can be made very expressive, I think they would need to be more intentional in how they spend memory and computational resources (using segmentation techniques, multi-scale resolutions, etc. to allow the model to capture actual patterns instead of simply letting attention figure everything out on its own). 
3. **Attention based methods can incur a lot of noise**: For some reason, I was always under the assumption that a "full" softmax attention was the most theoretically expressive, and that variants of the standard attention were all compromises that traded in capacity for reasons such as complexity and memory bottlenecks. While this is true in some instances, it is not always the case. For example, (Differential Transformer, Ye, Dong, Xia 2025) uses an attention map differential to dampen noise accumulated from the transformer and is claimed to be more theoretically expressive than the standard transformer. In my opinion, future directions for multivariate time series should introduce such methods in addition to other methods like signal processing techniques to address the noise problem in this domain.
4. **Relying too much on predictable patterns can up the performance initially but ultimately reduces the model to be less capable**: Time series data often exhibit cyclical, seasonal, and otherwise repeated patterns. While the discovery of such patterns is critically important, models can develop an over-reliance on them, which can ultimately hurt their expressivity, making them great on paper but useless in real-world scenarios. While this is an extremely complicated issue to tackle, I believe that part of the solution lies in *developing modular solutions for a generalist approach* (explored next in observation #5). 
5. **Most existing works develop end-to-end models starting with pre-processing steps and ending with decoding/predicting steps with each step dependent on the other**: However, time series data are incredibly diverse and can differ significantly across domains (traffic flow, electricity usage, climate modeling, financial data, etc.) not just in overall complexity of data but in the types of patterns they exhibit. As such, end-to-end models may be too inflexible. I believe that the answer is a modular approach that remains independent to, yet compatible with, various pre-processing and prediction/decoding blocks. With a modular learning block, pre-processing steps can be made as simple as linear projections to more complicated techniques such as Fourier and Wavelet transforms depending on the complexity of the domain-specific data. Likewise, prediction blocks can also be made as simple as a linear classifier/regressor to more heavy-duty models like decoder-only transformers. All of this is to say that unlike in many other domains, time-series (especially multivariate time series) would benefit from flexible architectures that can be as simple or complex as they need to be given domain-specific prior knowledge. I believe this approach could also help in balancing out the problems addressed in my observation #3 and #4. 